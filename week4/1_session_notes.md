# Decision Trees
Whenever we look at problems, we need to make a decision.  How do we make that decision? We need to establish data that we have.  

We are looking for an assessment on how accurate or confident that we are on the decisions that we make.  

Linear regression relies on linear models.  These are very powerful, however, sometimes we need to go to the next level. 
What are decision trees? This originally will look like a simple one step extension however this goes much further. 

## Definitions, Interpretations
What is a decision tree? 

Do you recieve a loan?

                  Yes (Loan) **Class 1**
                    /   
            Salary over $2500?  -----  No (No Loan) **Class 2**
            /
Age over 30? 
            \   
            Number of children > 2? -- No (No Loan) **Class 3**
                    \
                   Yes (Loan) **Class 4**

These features are comprised of Many different features and then outcomes.  
We start with the entire dataset.  Then essentially, we are splitting the data into multiple datasets.  There is no overlap.  
We are classifying groups from direct (In this case binary) questions within the dataset.

The "leaves" or final groups (end result) are labeled as different classes.

Each internal node represents a "test" or "features"
Each branch represents an outcome of a test. 

The description is generated by the "classification label" or everythiing that was confirmed to get to the final class.

### Advantages of Decision trees
- Simple to understand and interpret
- Understandable Machine Learning
- Mirrors Human choices (Very similar to how a human talks to a customer [think doctor to patient])
- Uses an open-box model (You can see the decisions vs compared to NNT ()
- Versatile
  - Able to handle both numerical and categorical
  - Can model arbitary functions
  - Requires little data preperation
  - performs well with large datasets
  - robust against "boosting"
- Built in feature selection
  - We naturally de-emphasize irrelevant features.  This is determined just by the fact of analyzing the data.  
  - The features are sorted based on their relevance to the data.
  - Develops a hierarchy in terms of relevance. 
- Testable : We can evaluate a model using statistical tests.

_what is NNT?_
_What is boosting?_

### Disadvantages of Decision trees
- Trees can be non-robust : Small change in training data can result in a large change in the tree and the final predictions)
- The problem of learning an optimal decision tree is known to be NP-Complete 
  - Practical decision-tree learning algorithms are based on greedy algorithm
  - This cannot guarantee to return the globally optimal decision tree.
- Overfitting 
  - Motivates **pruning and Random Forests**

**Good Book** : Classification and Regression Trees (Breiman and Stone)

### Classification 
Everything about decision trees are about classification.
Our goal is to take a dataset and classify data based on multiple categories. 
X's are Features and Y's are Outcomes. 

For now we will be focusing on a binary outcome of classification.

The training data is the set of data that **do** have outcomes.  
What we want to do is make a process that when recieving X, predicts Y.

Our goal is to create a process that has as little error as possible.  How do we measure the errors within our classifier? 
- Evaluate based on training data
- Then Evaluate based on data that the method has not seen before

Predictor vs Classifier
- In regression: 
  - predictory : Y Hat = g(x)
  - metric : E[(Y hat - Y)^2]

- In classification:
  - Classifier Y Hat = g(**x**) containing {1, ..., m}
  - metric : P(mistake)

### Type of Classifier : Linear 
- This classifier compares Theta^T @ X to a threshold
- This learns "good" vector Thetas and threshold.

We evaluate this by counting the number of points that were misclassified.

### Type of Classifier : Decision Tree

                  Yes (Loan) **Class 1**
                    /   
            Is X2 below or equal to this value?  ----->  No (No Loan) **Class 2**
                        /
is X1 to the left or right of this value? 

### Error Types and Confusion Matrix
- Binary Classification, we have two error types
- truth is "blue", Y Hat predicts "brown"
- truth is "brown", Y Hat predicts "blue"

We are interested in having few errors on new data records. 
There is a tradeoff between two error types, sometimes when looking at false positives vs false negatives, one will have a much more adverse effect when compared to another. 
There is asymetry based on this error.

### Numerical vs Categorical Data

Decision trees are simpler when variables are categorical because there are **finite outcomes** and the classification can often times be binary.

## Learning a Decision Tree
Lets look at a simple example. 
How to make the decision to wait for a table or leave. 
Leave will be the negative outcome, wait will be the postiive outcome.

**Attributes**:
- Whether there is a suitable alternative
- Is there a bar to wait in
- Friday or Saturday? 
- Are you hungry?
- How many poeple?
- The price
- Is it raining outside?
- Do you have a reservation?
- What type of resteraunt is it?
- What is the expected wait time? 

All of these attributes will be analyzed when determing whether or not we wait or stay at the restraunt.

Notation:
Feature Space- A vector of categorical data : _X_
Outcome Class (Categorical): _Y_
Decision Rule: _f_ : _X_ -> _Y_

### One Possible Tree: 

                  Yes (Wait) **Class 1**
                    /   
            Is it raining?  -----  No (Leave) **Class 2**
            /
    Reservation? 
            \   
            Are you hungry? -- No (Wait) **Class 3**
                    \
                   Yes (Leave) **Class 4**

How good is this decision tree?
We will be evaluating based on the missed calculation. 

Ideally this would be evaluated based on test data.
Count all of the data that is misclassified. 
This is known as the **Empirical Error** or the approximation of the likelyhood of making a mistake.

In this case, we would have **Misclassifications = 5/12**.
This error has to do with the underlying application. 
In this case, ~1/2 is quite a large error rate. 

If we are looking at something of high importance obviously we would want a much lower chance of error.

We can then generated a more Detailed decision tree. Say something that evaluates wait time as the first classifier, then determines how many people are in the area if 0-10 minutes, determines if you are hungry or not if the wait time is 10-60 minuts, and prompts to leave at 60 minutes and greater.

Probabilistic Description
Feature Space- A vector of Categorical Data: _X_
Outcome Class (Categorical): _Y_
A decision Rule _f_ : _X_ -> _Y_
A Probabilistic Model _X_ containing _X_ and _Y_ containing _Y_ are Random Variables
Error of decision rule: _f_ : R* ( _f_ ) = P {(X,Y) : f(X) != Y}

### How do we learn a decision tree from data? 
1. We split the data
 - Training Data
 - Cross Validation data
2. Train the model using the training data
3. Evaluate the performance on the "Cross Validation Data"
4. 80-20 ratio generally works! 
_How do we determine what category we use?_
5. Learn
  - Pick a feature
  - Split the data based on that feature
  - define new classes
  - Repeat
  - Label the final clusters using majority rule

How do we know how to stop? You can continue until you use all features or based on a level system using hyperparameters
When do we start with what feature? Order is important! 
This becomes the critical question.  

We want the first portion of data to be the most predictive.  We want what will **split** the data the best. 
You don't need to have all of the features within the model. 
Suppose that we have 10 features. 

On the first level we have 10 features we could use.
On the second level we have 9 features we could use.
etc. etc.

We have essentailly a factorial number of decision trees that we would be able to analyze based on the number of features.
This is very computationally difficult to optimize. 
This is what is known as an **np complete** problem, one that does not have a computationally sound way to complete it. 

We will find a certain nubmer of features until we eventually stop (run out or hyper parameters)
How do we label these final classes? 
We take the majority outcome of that class. 
Did the majority leave or stay? 
Then we can label that majority. 

If that group is not homogeneous, we would have a huge error! Our objective is to make teh subset as pure as possible.  The order we do this matters!! 

How to we determine what feature we pick?


## Information Gain
We do this with **Entropy** or **impurity** measure.  

### What is Entropy?
A greedy algorithm is a step-by-step alrogirhtm.
Entropy as a measure, measures randomness.  A higher value means more randomness, lower value means less.  
Entropy : H(Z) = -sum(z containing Z)(P(z)log(P(z))

Imagine we flip a coin:
Coin flip with P(head) = p
Entropy = -plogp-(1-p)log(1-p)

Entropys formula is based on the overall probability outcome.  
This is the probability times the log of that probability. 
We do this for both outcomes.  

We can then graph this and we can see the overall probability at each point of p. 
This is a measure of homogeonity.  

Lets look at another example
Y=0 -> Flu
Y=1 -> cold 
X=0 -> Low Fever
X=1 -> High Fever

    | Y=0 | Y=1
X=0 | 1/8 | 3/8 
X=1 | 3/8 | 1/8

H(Y) = H(1/2, 1/2) = 1
If we don't have a variable, it would be a 50/50

If someone walks in and says that the person has a fever, we can compare the probability outcome.  (3/8 vs 1/8)
Now we would be able to determine the overall conditional entropy.

H(Y|X=0) = -sum(P(y|x = 0) log P(y|x=0) = -1/4 log 1/4 - 3/4 log 3/4
H(Y|X=1) = -sum(P(y|x = 1) log P(y|x=1) = -3/4 log 3/4 - 3/4 log 1/4

### Information Gain
Our goal is to maximize the information gain or minimize the information gain of the conditional. 
Information Gain : IG(Y|X) = H(Y) - H(Y|X)

If X Y then IG = 0 X is not informative
f IG(Y|X) = H(Y) then Y is the most informative

We are searching for the feature that gives us the most information

Lets look at a simple boolean example.  

X1 | X2 | Y
 T |  T | T
 T |  F | T
 T |  T | T
 T |  F | T
 F |  T | T
 F |  F | F
 F |  T | F
 F |  F | F

X1 -> Y=t : 4, Y=f : 0
  \ 
 Y=t : 1, Y=f : 3

X2 -> Y=t : 3, Y=f : 1
  \ 
 Y=t : 2, Y=f : 2

X2 commits erorr on both sides, X1 however, splits the data well because we know that Y=t seperates perfectly based on X1.  

The greedy algorithm takes this and performs this for every single feature that we have.  
- STart with the complete data s
- Pick a feature (formally: X(m))
- Describe the data based on this feature
{(xi(m), yi), i=1, ..., N)}
- Split the outcome data based on the classes
S1 = {(yi|xi(m) = 0} | S2 = {(yi|xi(m) = 1}
- Empirically compute the conditional entropy
H(Y|X(m))
- Estimate:
P(s1)H(s1) + P(s2)H(s2)
- Pick m to minimize this (maximize information gain)
Feature splitting = Conditioning 

It is called greedy because we are optimizing based on each individual feature at a time. 

Lets run the computation for an example.

H(s1) = -2/5 log 2/5 - 3/5 log 3/5 = 0.97
H(s2) = -/37 log 3/7 - 4/7 log 4/7 = 0.99
Entropy of split 
0.97 * 5/12 + 0.99 * 7/12 = 0.979
IG = H(Y) - 0.979 = 1-0.979 = 0.021
This is a low value, therefore this is not a good split. 

### Gini Index
Very similar to entropy but slightly different
replaces -logP(x) with (1-p(x))
This favors larger partions and is much easier to compute.

Think of this as an _approximation_ while entropy is the more precise value. 

What do we do if we have a continuous variable? We need to cut it into different intervals.  We woudl do this with what is called **binning** or as a numerical value. 

